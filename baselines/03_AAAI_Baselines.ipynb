{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "03.AAAI-Baselines.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PthZlIj6MBdk",
        "X3O6psnaMn9x",
        "jOydLqBrNT9r",
        "tzeZ-TZmNX1u",
        "v4NzINPiNc19"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PthZlIj6MBdk"
      },
      "source": [
        "# ***AAAI22 MLC Paper - Baselines***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVp9qKLaISXT"
      },
      "source": [
        "This repository contains all the instructions to reproduce the state-of-the-art models that we use as baselines in our paper. We remind you that these repositories are maintained by the authors, so if any problem arises with their code, please add an issue in the corresponding repositories.\n",
        "\n",
        "Source codes:\n",
        "\n",
        "1. Layered: [https://github.com/meizhiju/layered-bilstm-crf](https://github.com/meizhiju/layered-bilstm-crf)\n",
        "2. Exhaustive: [https://github.com/csJd/deep_exhaustive_model](https://github.com/csJd/deep_exhaustive_model)\n",
        "3. Boundary: [https://github.com/thecharm/boundary-aware-nested-ner](https://github.com/thecharm/boundary-aware-nested-ner)\n",
        "4. Second-best: [https://github.com/yahshibu/nested-ner-tacl2020-flair](https://github.com/yahshibu/nested-ner-tacl2020-flair)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2GikXE2MA9N",
        "outputId": "02bb5eaf-a5f3-4016-d6a6-f1b35fff774e"
      },
      "source": [
        "# First, we set up the working environment in google drive. If you are working locally, it will not be necessary but make sure that you are using the GPU.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMDH8nP_3Jew",
        "outputId": "536d4585-ee92-4218-d36d-892ea087a961"
      },
      "source": [
        "# We will clone the repositories in the \"MyDrive\" folder.\n",
        "%cd gdrive/MyDrive/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wGhxSnUMzZi"
      },
      "source": [
        "# We create a folder where we will clone each repository. If the folder is already created, then skip this step.\n",
        "!mkdir mlc-paper-baselines"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3O6psnaMn9x"
      },
      "source": [
        "# ***Layered model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t_ab80Qun4S"
      },
      "source": [
        "# We advance to the folder where we will save the baselines.\n",
        "%cd mlc-paper-baselines/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eoNNFiHNTff"
      },
      "source": [
        "# Clone the project from the official repository. If you have already cloned it, skip this step.\n",
        "!git clone https://github.com/meizhiju/layered-bilstm-crf.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jUVwJwRNtb_"
      },
      "source": [
        "# Navigate to the \"src\" folder. This folder contains the main scripts with which we will train and test this model.\n",
        "%cd layered-bilstm-crf/src/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP_iU3JOOFpl"
      },
      "source": [
        "On the left side of Google Colab, you can navigate to the folder containing the project (MyDrive > mlc-paper-baselines > layered-bilstm-crf > src). To execute this model, we had to make the following changes to the source code:\n",
        "\n",
        "1. **Data loading**: In the folder named \"dataset\", put the files train.data, dev.data and test.data. Make sure that you follow the format explained in the repository. \n",
        "\n",
        "2. **Embeddings**: Create a folder named \"embeddings\" inside the src folder, and put the embeddings described in the repository.\n",
        "\n",
        "3. **Hyperparameters**: Change the hyperparameters and directories in the config file (add: ../src/embeddings/embedding_name to the path_pre_emb param). If you are working with the Chilean Waiting List corpus, remember that these clinical embeddings have a dimension of 300. If you are working with a GPU, set the 'main' key to 0, and if you are training, set the mode key to 'train'. As an example in the Chilean corpus:\n",
        "\n",
        "\n",
        "```\n",
        "- word_embedding_dim: 300 \n",
        "- char_embedding_dim: 25\n",
        "- dropout_ratio: 0.3\n",
        "- lr_param: 0.001\n",
        "- threshold: 5 \n",
        "- decay_rate: 0\n",
        "- batch_size: 16\n",
        "- tags: 7 \n",
        "- epochs: 20\n",
        "- replace_digit: false\n",
        "- lowercase: false\n",
        "- use_singletons: false\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4. **Code fixes**: Go to the layered_model.py script in the model folder. Due to an error generated by the class called \"Evaluator\", you must make the following two changes in that class:\n",
        "\n",
        "```\n",
        "1. Before (line 397): def __init__(self, iterator, target, device): \n",
        "   Now: def __init__(self, iterator, target, device=cuda.cupy):\n",
        "\n",
        "2. Before (line 398): super(Evaluator, self).__init__(iterator=iterator, target=target, device=device) \n",
        "   Now: super(Evaluator, self).__init__(iterator=iterator, target=target)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBDXeKjLRf78"
      },
      "source": [
        "# We install the repository dependencies.\n",
        "!pip install chainer\n",
        "!pip install texttable\n",
        "!pip install 'cupy-cuda101>=7.7.0,<8.0.0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmUI11j0Rc6Z"
      },
      "source": [
        "# Next, we train the Layered model.\n",
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms_IG9ynTWkM"
      },
      "source": [
        "To evaluate, change the training mode to test in the configuration file and specify the path to the best model located in the \"result\" folder using the \"path_model\" key. In the \"evaluation\" folder will be found the file with the output. Then, we can calculate the task-specific metrics using this file (go to the metrics Jupyter notebook). Also, in utils.py in the \"model\" folder, you have to comment on the following lines to save the predictions file:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "1. Before (line 202): os.remove(output_path)\n",
        "   Now: #os.remove(output_path)\n",
        "\n",
        "2. Before (line 203): os.remove(scores_path)\n",
        "   Now: #os.remove(scores_path)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7tKBXqxTkyC"
      },
      "source": [
        "# And we run the best model calculated on the validation set, now on our testing data. The file with the predictions will be located in the \"evaluation\" folder with the extension '.scores'.\n",
        "!python test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOydLqBrNT9r"
      },
      "source": [
        "# ***Exhaustive model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itk8XpVJTARJ"
      },
      "source": [
        "# If you were in the layered folder\n",
        "#%cd ..\n",
        "#%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ymfuYj_S9Fy"
      },
      "source": [
        "# We advance to the folder where we will save the baselines.\n",
        "%cd mlc-paper-baselines/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q95tho4q67Nt"
      },
      "source": [
        "# Clone the project from the official repository. If you have already cloned it, skip this step.\n",
        "!git clone https://github.com/csJd/deep_exhaustive_model.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5PwoDJhTSXM"
      },
      "source": [
        "# In the folder called \"data\", we will create a folder for each dataset. We also create a folder for the models.\n",
        "%cd deep_exhaustive_model/data\n",
        "!mkdir genia\n",
        "!mkdir wl\n",
        "!mkdir germ\n",
        "!mkdir model\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZoSyOqPpbfl"
      },
      "source": [
        "On the left side of Google Colab, you can navigate to the folder containing the project (MyDrive > mlc-paper-baselines > deep_exhaustive_model). To execute this model, we had to make the following changes to the source code:\n",
        "\n",
        "For example using the Chilean Waiting List dataset.\n",
        "\n",
        "1. **Data loading**: In the folder named \"data\", wl.put the files train.iob2, wl.dev.iob2 and wl.test.iob2 in the wl folder, following the format explained in the repository (As an example, we will use the case of the waiting list).\n",
        "\n",
        "2. **Embeddings**: Put the embeddings described in the repository in the \"embedding\" folder, which is inside the \"data\" folder.\n",
        "\n",
        "3. **Hyperparameters**: Change the hyperparameters and directories in the \"train.py\" script. If you are working with the Chilean Waiting List corpus, remember that these clinical embeddings have a dimension of 300 (line 90). As an example in the Chilean corpus:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "- embedding_dim: 300 \n",
        "- char_feat_dim: 25\n",
        "- learning_rate: 0.001\n",
        "- clip_norm: 5 \n",
        "- batch_size: 16\n",
        "- epochs: 30\n",
        "- hidden_size = 128\n",
        "```\n",
        "\n",
        "4. **Code fixes**: You must make the following changes in lines 24 to 28.\n",
        "\n",
        "```\n",
        "EMBD_URL = from_project_root(\"data/embedding/cwlce.vec\")\n",
        "VOCAB_URL = from_project_root(\"data/wl/vocab.json\")\n",
        "TRAIN_URL = from_project_root(\"data/wl/wl.train.iob2\")\n",
        "DEV_URL = from_project_root(\"data/wl/wl.dev.iob2\")\n",
        "TEST_URL = from_project_root(\"data/wl/wl.test.iob2\")\n",
        "```\n",
        "\n",
        "In the \"dataset.py script\", you have to change \"data_urls\" (line 272) variable to: \n",
        "\n",
        "```\n",
        "data_urls = [from_project_root(\"data/wl/wl.train.iob2\"),\n",
        "                 from_project_root(\"data/wl/wl.dev.iob2\"),\n",
        "                 from_project_root(\"data/wl/wl.test.iob2\")]\n",
        "```\n",
        "\n",
        "Add the pre-trained embedding path (line 275)\n",
        "```\n",
        "prepare_vocab(data_urls, from_project_root(\"data/embedding/cwlce.vec\"), update=True, min_count=1)\n",
        "```\n",
        "\n",
        "Finally, in \"model.py\", change the CharLSTM class (line 101), use the following:\n",
        "\n",
        "\n",
        "```\n",
        "class CharLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_chars, embedding_size, hidden_size, lstm_layers=1, bidirectional=True):\n",
        "        super().__init__()\n",
        "        self.n_chars = n_chars\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_hidden = hidden_size * (1 + bidirectional)\n",
        "\n",
        "        self.embedding = nn.Embedding(n_chars, embedding_size, padding_idx=0)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_size,\n",
        "            hidden_size=hidden_size,\n",
        "            bidirectional=bidirectional,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def sent_forward(self, words, lengths, indices):\n",
        "        sent_len = words.shape[0]\n",
        "        # words shape: (sent_len, max_word_len)\n",
        "\n",
        "        embedded = self.embedding(words)\n",
        "        # in_data shape: (sent_len, max_word_len, embedding_dim)\n",
        "\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu().numpy(), batch_first=True)\n",
        "        _, (hn, _) = self.lstm(packed)\n",
        "        # shape of hn:  (n_layers * n_directions, sent_len, hidden_size)\n",
        "\n",
        "        hn = hn.permute(1, 0, 2).contiguous().view(sent_len, -1)\n",
        "        hn2 = hn.clone()\n",
        "        # shape of hn:  (sent_len, n_layers * n_directions * hidden_size) = (sent_len, 2*hidden_size)\n",
        "\n",
        "        # shape of indices: (sent_len, max_word_len)\n",
        "        hn[indices] = hn2  # unsort hn\n",
        "        # unsorted = hn.new_empty(hn.size())\n",
        "        # unsorted.scatter_(dim=0, index=indices.unsqueeze(-1).expand_as(hn), src=hn)\n",
        "        return hn\n",
        "\n",
        "    def forward(self, sentence_words, sentence_word_lengths, sentence_word_indices):\n",
        "        # sentence_words [batch_size, *sent_len, max_word_len]\n",
        "        # sentence_word_lengths [batch_size, *sent_len]\n",
        "        # sentence_word_indices [batch_size, *sent_len, max_word_len]\n",
        "\n",
        "        batch_size = len(sentence_words)\n",
        "        batch_char_feat = torch.nn.utils.rnn.pad_sequence(\n",
        "            [self.sent_forward(sentence_words[i], sentence_word_lengths[i], sentence_word_indices[i])\n",
        "             for i in range(batch_size)], batch_first=True)\n",
        "\n",
        "        return batch_char_feat\n",
        "        # (batch_size, sent_len, 2 * hidden_size)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgT-Hr6Prvgj"
      },
      "source": [
        "# Now, we create the dataset pickle files.\n",
        "!python dataset.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGrZtJwntoAz"
      },
      "source": [
        "# And we train the Exhaustive model.\n",
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdq_VkBQwKW3"
      },
      "source": [
        "To evaluate and generate the prediction file, in the eval.py script, uncomment the following line (line 144): \n",
        "\n",
        "```\n",
        "# predict_on_iob2(model, test_url)\n",
        "```\n",
        "\n",
        "Change the model_url to the best model found (line 139):\n",
        "\n",
        "```\n",
        "model_url = from_project_root(\"data/model/model.pt\")\n",
        "```\n",
        "\n",
        "And, change the test_url (line 143):\n",
        "\n",
        "\n",
        "```\n",
        "test_url = from_project_root(\"data/wl/wl.test.iob2\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU2CONfAws4O"
      },
      "source": [
        "!python eval.py"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfIqwN2TwzEO"
      },
      "source": [
        "In the corpus folder (wl in this example) will be located the wl.test.pred.txt file with the output, to which the metrics can be calculated in our notebook of task-specific metrics. Note that in this output, the multilabel entities are not considered. Therefore, the Chilean Waiting List predictions must be compared with the original data to obtain the objective metric. This is done in the metrics notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzeZ-TZmNX1u"
      },
      "source": [
        "# ***Boundary model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBfRj-RXXiNc",
        "outputId": "8d9f9ecb-563f-482b-aac6-0ff7b3ea36ca"
      },
      "source": [
        "# If you were in the exhaustive folder\n",
        "# %cd ..\n",
        "# %cd .."
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvgpBj1yT3Fu"
      },
      "source": [
        "# Clone the project from the official repository. If you have already cloned it, skip this step.\n",
        "%cd mlc-paper-baselines/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0-1AtW3Naos"
      },
      "source": [
        "# Clone the project from the unofficial repository, if you have already cloned it skip this step.\n",
        "!git clone https://github.com/thecharm/boundary-aware-nested-ner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZPHldiwvHyh",
        "outputId": "3e093c78-65b8-4d09-a313-a459c4332ca3"
      },
      "source": [
        "%cd boundary-aware-nested-ner/Our_boundary-aware_model/"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VFHMZgN211N",
        "outputId": "75fd4676-b44f-4915-f49a-15f05694bc08"
      },
      "source": [
        "# In the folder called \"data\", we will create a folder for each dataset. We also create a folder for the models and the embeddings folder.\n",
        "%cd data\n",
        "!mkdir wl\n",
        "!mkdir germ\n",
        "!mkdir model\n",
        "!mkdir embedding\n",
        "%cd .."
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model/data\n",
            "/content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1LeIlxLy81_"
      },
      "source": [
        "On the left side of Google Colab, you can navigate to the folder containing the project (MyDrive > mlc-paper-baselines > boundary-aware-nested-ner > Our_boundary-aware_model). To execute this model, we had to make the following changes to the source code:\n",
        "\n",
        "For example using the Chilean Waiting List dataset.\n",
        "\n",
        "1. **Data loading**: In the \"data\" folder, place the files wl.train.iob2, wl.dev.iob2 and wl.test.iob2 in the wl folder, following the format explained in the repository. (As an example, we will use the case of the waiting list).\n",
        "\n",
        "2. **Embeddings**: Put the embeddings described in the repository in the \"embedding\" folder, which is inside the \"data\" folder.\n",
        "\n",
        "3. **Hyperparameters**: Change the hyperparameters and directories in the \"train.py\" script. If you are working with the Chilean Waiting List corpus, remember that these clinical embeddings have a dimension of 300 (line 95). \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "MAX_REGION = 10\n",
        "EARLY_STOP = 5\n",
        "LR = 0.005\n",
        "BATCH_SIZE = 16 \n",
        "MAX_GRAD_NORM = 5\n",
        "N_TAGS = 8\n",
        "TAG_WEIGHTS = [1, 1, 1, 1, 1, 1, 1, 1]\n",
        "FREEZE_WV = False\n",
        "LOG_PER_BATCH = 10\n",
        "hidden_size=128,\n",
        "lstm_layers=3\n",
        "```\n",
        "\n",
        "\n",
        "4. **Code fixes**\n",
        "\n",
        "Lines 34-38\n",
        "\n",
        "```\n",
        "PRETRAINED_URL = from_project_root(\"data/embedding/cwlce.vec\")\n",
        "EMBED_URL = from_project_root(\"data/wl/embeddings.npy\")\n",
        "VOCAB_URL = from_project_root(\"data/wl/vocab.json\")\n",
        "TRAIN_URL = from_project_root(\"data/wl/wl.train.iob2\")\n",
        "DEV_URL = from_project_root(\"data/wl/wl.dev.iob2\")\n",
        "TEST_URL = from_project_root(\"data/wl/wl.test.iob2\")\n",
        "```\n",
        "\n",
        "Change tags and weights\n",
        "```\n",
        "N_TAGS = 8 (line 29)\n",
        "TAG_WEIGHTS = [1, 1, 1, 1, 1, 1, 1, 1] (line 30)\n",
        "```\n",
        "\n",
        "And delete the following lines:\n",
        "```\n",
        "import pdb (line 21)\n",
        "pdb.set_trace() (line 22)\n",
        "```\n",
        "\n",
        "In the \"dataset.py\" script, change data_urls (line 286) to \n",
        "\n",
        "```\n",
        "data_urls = [from_project_root(\"data/wl/wl.train.iob2\"),\n",
        "                 from_project_root(\"data/wl/wl.dev.iob2\"),\n",
        "                 from_project_root(\"data/wl/wl.test.iob2\")]\n",
        "```\n",
        "\n",
        "Add the pre-trained embedding path (line 289)\n",
        "```\n",
        "prepare_vocab(data_urls, from_project_root(\"data/embedding/cwlce.vec\"), update=True, min_count=1)\n",
        "```\n",
        "\n",
        "Change these three variables (lines 15-17):\n",
        "```\n",
        "LABEL_IDS = {\"neither\": 0, \"Disease\": 1, \"Finding\": 2, \"Medication\": 3, \"Procedure\": 4, \"Family_Member\": 5,\"Body_Part\": 6, \"Abbreviation\": 7}\n",
        "PRETRAINED_URL = from_project_root(\"data/embedding/cwlce.vec\")\n",
        "LABEL_LIST = {\"O\", \"Disease\", \"Finding\", \"Medication\", \"Procedure\", \"Family_Member\",\"Body_Part\", \"Abbreviation\"}\n",
        "```\n",
        "\n",
        "Finally, in \"model.py\", change the CharLSTM class, use the following:\n",
        "\n",
        "```\n",
        "class CharLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_chars, embedding_size, hidden_size, lstm_layers=1, bidirectional=True):\n",
        "        super().__init__()\n",
        "        self.n_chars = n_chars\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_hidden = hidden_size * (1 + bidirectional)\n",
        "\n",
        "        self.embedding = nn.Embedding(n_chars, embedding_size, padding_idx=0)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_size,\n",
        "            hidden_size=hidden_size,\n",
        "            bidirectional=bidirectional,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def sent_forward(self, words, lengths, indices):\n",
        "        sent_len = words.shape[0]\n",
        "        # words shape: (sent_len, max_word_len)\n",
        "\n",
        "        embedded = self.embedding(words)\n",
        "        # in_data shape: (sent_len, max_word_len, embedding_dim)\n",
        "\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu().numpy(), batch_first=True)\n",
        "        _, (hn, _) = self.lstm(packed)\n",
        "        # shape of hn:  (n_layers * n_directions, sent_len, hidden_size)\n",
        "\n",
        "        hn = hn.permute(1, 0, 2).contiguous().view(sent_len, -1)\n",
        "        hn2 = hn.clone()\n",
        "        # shape of hn:  (sent_len, n_layers * n_directions * hidden_size) = (sent_len, 2*hidden_size)\n",
        "\n",
        "        # shape of indices: (sent_len, max_word_len)\n",
        "        hn[indices] = hn2  # unsort hn\n",
        "        # unsorted = hn.new_empty(hn.size())\n",
        "        # unsorted.scatter_(dim=0, index=indices.unsqueeze(-1).expand_as(hn), src=hn)\n",
        "        return hn\n",
        "\n",
        "    def forward(self, sentence_words, sentence_word_lengths, sentence_word_indices):\n",
        "        # sentence_words [batch_size, *sent_len, max_word_len]\n",
        "        # sentence_word_lengths [batch_size, *sent_len]\n",
        "        # sentence_word_indices [batch_size, *sent_len, max_word_len]\n",
        "\n",
        "        batch_size = len(sentence_words)\n",
        "        batch_char_feat = torch.nn.utils.rnn.pad_sequence(\n",
        "            [self.sent_forward(sentence_words[i], sentence_word_lengths[i], sentence_word_indices[i])\n",
        "             for i in range(batch_size)], batch_first=True)\n",
        "\n",
        "        return batch_char_feat\n",
        "        # (batch_size, sent_len, 2 * hidden_size)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl6KX2rOyzOA",
        "outputId": "eddbf9fd-e0b6-497c-a9b4-1ae12274f4a2"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generating vocab from ['/content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model/data/wl/wl.train.iob2', '/content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model/data/wl/wl.dev.iob2', '/content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model/data/wl/wl.test.iob2']\n",
            "generating pre-trained embedding from /content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model/data/embedding/cwlce.vec\n",
            "arguments {\n",
            "  \"n_epochs\": 30,\n",
            "  \"embedding_url\": \"/content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model/data/wl/embeddings.npy\",\n",
            "  \"char_feat_dim\": 50,\n",
            "  \"freeze\": false,\n",
            "  \"train_url\": \"/content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model/data/wl/wl.train.iob2\",\n",
            "  \"dev_url\": \"/content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model/data/wl/wl.dev.iob2\",\n",
            "  \"test_url\": \"/content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model/data/wl/wl.test.iob2\",\n",
            "  \"learning_rate\": 0.005,\n",
            "  \"batch_size\": 16,\n",
            "  \"early_stop\": 5,\n",
            "  \"clip_norm\": 5,\n",
            "  \"bsl_model_url\": null,\n",
            "  \"gamma\": 0.3,\n",
            "  \"device\": \"auto\",\n",
            "  \"save_only_best\": true\n",
            "}\n",
            "using gpu, 1 gpu(s) available!\n",
            "\n",
            "/content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model/dataset.py:116: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  words = np.array(words)[word_indices.cpu().numpy()]\n",
            "epoch #0, batch #0, loss: 1.584727525711, 03:36:32\n",
            "epoch #0, batch #10, loss: 1.337104558945, 03:36:33\n",
            "epoch #0, batch #20, loss: 1.293934106827, 03:36:34\n",
            "epoch #0, batch #30, loss: 1.197921872139, 03:36:35\n",
            "epoch #0, batch #40, loss: 1.146252512932, 03:36:36\n",
            "epoch #0, batch #50, loss: 1.234969615936, 03:36:38\n",
            "epoch #0, batch #60, loss: 0.988789319992, 03:36:39\n",
            "epoch #0, batch #70, loss: 1.052646636963, 03:36:40\n",
            "epoch #0, batch #80, loss: 0.911210536957, 03:36:41\n",
            "epoch #0, batch #90, loss: 1.008000731468, 03:36:42\n",
            "epoch #0, batch #100, loss: 0.811206221581, 03:36:43\n",
            "epoch #0, batch #110, loss: 0.792173624039, 03:36:44\n",
            "epoch #0, batch #120, loss: 0.770614147186, 03:36:45\n",
            "epoch #0, batch #130, loss: 0.719420790672, 03:36:46\n",
            "epoch #0, batch #140, loss: 0.751775622368, 03:36:48\n",
            "epoch #0, batch #150, loss: 0.653154373169, 03:36:49\n",
            "epoch #0, batch #160, loss: 0.597333431244, 03:36:50\n",
            "epoch #0, batch #170, loss: 0.587648391724, 03:36:51\n",
            "epoch #0, batch #180, loss: 0.633358359337, 03:36:52\n",
            "epoch #0, batch #190, loss: 0.511283457279, 03:36:53\n",
            "epoch #0, batch #200, loss: 0.637860655785, 03:36:54\n",
            "epoch #0, batch #210, loss: 0.437488198280, 03:36:56\n",
            "epoch #0, batch #220, loss: 0.513166487217, 03:36:57\n",
            "epoch #0, batch #230, loss: 0.540826201439, 03:36:58\n",
            "epoch #0, batch #240, loss: 0.525621592999, 03:36:59\n",
            "epoch #0, batch #250, loss: 0.469775080681, 03:37:00\n",
            "epoch #0, batch #260, loss: 0.505801439285, 03:37:01\n",
            "epoch #0, batch #270, loss: 0.376952946186, 03:37:02\n",
            "epoch #0, batch #280, loss: 0.428241431713, 03:37:03\n",
            "epoch #0, batch #290, loss: 0.379669249058, 03:37:04\n",
            "epoch #0, batch #300, loss: 0.411974191666, 03:37:05\n",
            "epoch #0, batch #310, loss: 0.445742249489, 03:37:07\n",
            "epoch #0, batch #320, loss: 0.409885227680, 03:37:08\n",
            "epoch #0, batch #330, loss: 0.412654757500, 03:37:09\n",
            "epoch #0, batch #340, loss: 0.381126642227, 03:37:10\n",
            "epoch #0, batch #350, loss: 0.357099175453, 03:37:11\n",
            "epoch #0, batch #360, loss: 0.411342352629, 03:37:12\n",
            "epoch #0, batch #370, loss: 0.403117567301, 03:37:13\n",
            "epoch #0, batch #380, loss: 0.414306968451, 03:37:14\n",
            "epoch #0, batch #390, loss: 0.397317618132, 03:37:15\n",
            "epoch #0, batch #400, loss: 0.416757524014, 03:37:17\n",
            "epoch #0, batch #410, loss: 0.359691321850, 03:37:18\n",
            "epoch #0, batch #420, loss: 0.368699252605, 03:37:19\n",
            "epoch #0, batch #430, loss: 0.314532279968, 03:37:20\n",
            "epoch #0, batch #440, loss: 0.371023952961, 03:37:21\n",
            "epoch #0, batch #450, loss: 0.341656625271, 03:37:22\n",
            "epoch #0, batch #460, loss: 0.384710907936, 03:37:23\n",
            "epoch #0, batch #470, loss: 0.412331104279, 03:37:24\n",
            "epoch #0, batch #480, loss: 0.255063980818, 03:37:26\n",
            "epoch #0, batch #490, loss: 0.363341867924, 03:37:27\n",
            "epoch #0, batch #500, loss: 0.281148076057, 03:37:28\n",
            "\n",
            "evaluating model on: /content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model/data/wl/wl.dev.iob2 \n",
            "\n",
            "sentence head and tail labeling result:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  out-entity   0.992628  0.988976  0.990799     80192\n",
            " head-entity   0.837299  0.860330  0.848658      3272\n",
            " tail-entity   0.807821  0.794069  0.800886      1821\n",
            "   in-entity   0.770856  0.823495  0.796307      3456\n",
            "\n",
            "    accuracy                       0.973789     88741\n",
            "   macro avg   0.852151  0.866718  0.859163     88741\n",
            "weighted avg   0.974472  0.973789  0.974086     88741\n",
            "\n",
            "region classification result:\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      neither   0.000000  0.000000  0.000000       576\n",
            "      Disease   0.763407  0.510011  0.611497       949\n",
            "      Finding   0.474333  0.239875  0.318621       963\n",
            "   Medication   0.183206  0.600000  0.280702        80\n",
            "    Procedure   0.375758  0.196825  0.258333       315\n",
            "Family_Member   0.000000  0.000000  0.000000        29\n",
            "    Body_Part   0.662768  0.529595  0.588745       642\n",
            " Abbreviation   0.773956  0.858311  0.813953       734\n",
            "\n",
            "     accuracy                       0.418610      4288\n",
            "    macro avg   0.404178  0.366827  0.358981      4288\n",
            " weighted avg   0.538213  0.418610  0.458580      4288\n",
            "\n",
            " precision = 0.624348, recall = 0.503365, micro_f1 = 0.557367\n",
            "\n",
            "maximum of f1 value: 0.557367, in epoch #0\n",
            "training time: 0:01:06\n",
            "Wed Sep  1 03:37:34 2021\n",
            "\n",
            "epoch #1, batch #0, loss: 0.330096244812, 03:37:34\n",
            "epoch #1, batch #10, loss: 0.406303942204, 03:37:35\n",
            "epoch #1, batch #20, loss: 0.423871397972, 03:37:37\n",
            "epoch #1, batch #30, loss: 0.263966947794, 03:37:38\n",
            "epoch #1, batch #40, loss: 0.283537626266, 03:37:39\n",
            "epoch #1, batch #50, loss: 0.384511053562, 03:37:40\n",
            "epoch #1, batch #60, loss: 0.239435285330, 03:37:41\n",
            "epoch #1, batch #70, loss: 0.346978396177, 03:37:42\n",
            "epoch #1, batch #80, loss: 0.346899867058, 03:37:43\n",
            "epoch #1, batch #90, loss: 0.356120944023, 03:37:44\n",
            "epoch #1, batch #100, loss: 0.246499732137, 03:37:46\n",
            "epoch #1, batch #110, loss: 0.296643733978, 03:37:47\n",
            "epoch #1, batch #120, loss: 0.326225221157, 03:37:48\n",
            "epoch #1, batch #130, loss: 0.307795375586, 03:37:49\n",
            "epoch #1, batch #140, loss: 0.257703125477, 03:37:50\n",
            "epoch #1, batch #150, loss: 0.297805160284, 03:37:51\n",
            "epoch #1, batch #160, loss: 0.287057220936, 03:37:52\n",
            "epoch #1, batch #170, loss: 0.229402750731, 03:37:53\n",
            "epoch #1, batch #180, loss: 0.252061724663, 03:37:54\n",
            "epoch #1, batch #190, loss: 0.199899986386, 03:37:55\n",
            "epoch #1, batch #200, loss: 0.374790817499, 03:37:57\n",
            "epoch #1, batch #210, loss: 0.148205250502, 03:37:58\n",
            "epoch #1, batch #220, loss: 0.260171055794, 03:37:59\n",
            "epoch #1, batch #230, loss: 0.257938683033, 03:38:00\n",
            "epoch #1, batch #240, loss: 0.243041574955, 03:38:01\n",
            "epoch #1, batch #250, loss: 0.179407417774, 03:38:02\n",
            "epoch #1, batch #260, loss: 0.282818615437, 03:38:04\n",
            "epoch #1, batch #270, loss: 0.196822583675, 03:38:05\n",
            "epoch #1, batch #280, loss: 0.219297036529, 03:38:06\n",
            "epoch #1, batch #290, loss: 0.195935398340, 03:38:07\n",
            "epoch #1, batch #300, loss: 0.221214562654, 03:38:08\n",
            "epoch #1, batch #310, loss: 0.255344152451, 03:38:09\n",
            "epoch #1, batch #320, loss: 0.237021356821, 03:38:10\n",
            "epoch #1, batch #330, loss: 0.235498189926, 03:38:11\n",
            "epoch #1, batch #340, loss: 0.231560245156, 03:38:12\n",
            "epoch #1, batch #350, loss: 0.200452685356, 03:38:13\n",
            "epoch #1, batch #360, loss: 0.277090072632, 03:38:15\n",
            "epoch #1, batch #370, loss: 0.211064755917, 03:38:16\n",
            "epoch #1, batch #380, loss: 0.248163789511, 03:38:17\n",
            "epoch #1, batch #390, loss: 0.267186045647, 03:38:18\n",
            "epoch #1, batch #400, loss: 0.255755841732, 03:38:19\n",
            "epoch #1, batch #410, loss: 0.201765358448, 03:38:20\n",
            "epoch #1, batch #420, loss: 0.234074980021, 03:38:21\n",
            "epoch #1, batch #430, loss: 0.194441854954, 03:38:23\n",
            "epoch #1, batch #440, loss: 0.196292549372, 03:38:24\n",
            "epoch #1, batch #450, loss: 0.199907928705, 03:38:25\n",
            "epoch #1, batch #460, loss: 0.237771779299, 03:38:26\n",
            "epoch #1, batch #470, loss: 0.248556613922, 03:38:27\n",
            "epoch #1, batch #480, loss: 0.152714878321, 03:38:28\n",
            "epoch #1, batch #490, loss: 0.214385062456, 03:38:29\n",
            "epoch #1, batch #500, loss: 0.186822146177, 03:38:30\n",
            "\n",
            "evaluating model on: /content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model/data/wl/wl.dev.iob2 \n",
            "\n",
            "sentence head and tail labeling result:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  out-entity   0.991728  0.991184  0.991456     80192\n",
            " head-entity   0.857313  0.881418  0.869198      3272\n",
            " tail-entity   0.814056  0.807798  0.810915      1821\n",
            "   in-entity   0.808884  0.800926  0.804885      3456\n",
            "\n",
            "    accuracy                       0.975964     88741\n",
            "   macro avg   0.867995  0.870331  0.869114     88741\n",
            "weighted avg   0.976005  0.975964  0.975977     88741\n",
            "\n",
            "region classification result:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      neither   0.000000  0.000000  0.000000       793\n",
            "      Disease   0.655771  0.676502  0.665975       949\n",
            "      Finding   0.556509  0.403946  0.468111       963\n",
            "   Medication   0.305389  0.637500  0.412955        80\n",
            "    Procedure   0.580786  0.422222  0.488971       315\n",
            "Family_Member   1.000000  0.379310  0.550000        29\n",
            "    Body_Part   0.724437  0.651090  0.685808       642\n",
            " Abbreviation   0.813049  0.882834  0.846506       734\n",
            "\n",
            "     accuracy                       0.508768      4505\n",
            "    macro avg   0.579493  0.506676  0.514791      4505\n",
            " weighted avg   0.545281  0.508768  0.521074      4505\n",
            "\n",
            " precision = 0.662619, recall = 0.642737, micro_f1 = 0.652527\n",
            "\n",
            "maximum of f1 value: 0.652527, in epoch #1\n",
            "training time: 0:02:08\n",
            "Wed Sep  1 03:38:37 2021\n",
            "\n",
            "epoch #2, batch #0, loss: 0.219633400440, 03:38:37\n",
            "epoch #2, batch #10, loss: 0.254970878363, 03:38:38\n",
            "epoch #2, batch #20, loss: 0.329405784607, 03:38:39\n",
            "epoch #2, batch #30, loss: 0.168618142605, 03:38:40\n",
            "epoch #2, batch #40, loss: 0.204936414957, 03:38:41\n",
            "epoch #2, batch #50, loss: 0.292035341263, 03:38:43\n",
            "epoch #2, batch #60, loss: 0.146700173616, 03:38:44\n",
            "epoch #2, batch #70, loss: 0.258866131306, 03:38:45\n",
            "epoch #2, batch #80, loss: 0.182738393545, 03:38:46\n",
            "epoch #2, batch #90, loss: 0.252272844315, 03:38:47\n",
            "epoch #2, batch #100, loss: 0.154220953584, 03:38:48\n",
            "epoch #2, batch #110, loss: 0.235076636076, 03:38:50\n",
            "epoch #2, batch #120, loss: 0.236908555031, 03:38:50\n",
            "epoch #2, batch #130, loss: 0.234399870038, 03:38:52\n",
            "epoch #2, batch #140, loss: 0.162782907486, 03:38:53\n",
            "epoch #2, batch #150, loss: 0.203862011433, 03:38:54\n",
            "epoch #2, batch #160, loss: 0.203296601772, 03:38:55\n",
            "epoch #2, batch #170, loss: 0.169782489538, 03:38:56\n",
            "epoch #2, batch #180, loss: 0.186399638653, 03:38:57\n",
            "epoch #2, batch #190, loss: 0.141770839691, 03:38:58\n",
            "epoch #2, batch #200, loss: 0.349645644426, 03:38:59\n",
            "epoch #2, batch #210, loss: 0.092761345208, 03:39:01\n",
            "epoch #2, batch #220, loss: 0.195499002934, 03:39:02\n",
            "epoch #2, batch #230, loss: 0.176710978150, 03:39:03\n",
            "epoch #2, batch #240, loss: 0.173377692699, 03:39:04\n",
            "epoch #2, batch #250, loss: 0.133511275053, 03:39:05\n",
            "epoch #2, batch #260, loss: 0.228033602238, 03:39:06\n",
            "epoch #2, batch #270, loss: 0.162359327078, 03:39:07\n",
            "epoch #2, batch #280, loss: 0.156884372234, 03:39:08\n",
            "epoch #2, batch #290, loss: 0.116360187531, 03:39:10\n",
            "epoch #2, batch #300, loss: 0.171060174704, 03:39:11\n",
            "epoch #2, batch #310, loss: 0.199057668447, 03:39:12\n",
            "epoch #2, batch #320, loss: 0.182135581970, 03:39:13\n",
            "epoch #2, batch #330, loss: 0.202358946204, 03:39:14\n",
            "epoch #2, batch #340, loss: 0.198719829321, 03:39:15\n",
            "epoch #2, batch #350, loss: 0.158129453659, 03:39:16\n",
            "epoch #2, batch #360, loss: 0.234774038196, 03:39:17\n",
            "epoch #2, batch #370, loss: 0.159656226635, 03:39:18\n",
            "epoch #2, batch #380, loss: 0.196050271392, 03:39:19\n",
            "epoch #2, batch #390, loss: 0.211329445243, 03:39:20\n",
            "epoch #2, batch #400, loss: 0.197284981608, 03:39:22\n",
            "epoch #2, batch #410, loss: 0.173115372658, 03:39:23\n",
            "epoch #2, batch #420, loss: 0.179823309183, 03:39:24\n",
            "epoch #2, batch #430, loss: 0.151587843895, 03:39:25\n",
            "epoch #2, batch #440, loss: 0.137056156993, 03:39:26\n",
            "epoch #2, batch #450, loss: 0.148722141981, 03:39:27\n",
            "epoch #2, batch #460, loss: 0.182620346546, 03:39:28\n",
            "epoch #2, batch #470, loss: 0.229714110494, 03:39:29\n",
            "epoch #2, batch #480, loss: 0.121764808893, 03:39:31\n",
            "epoch #2, batch #490, loss: 0.167229935527, 03:39:32\n",
            "epoch #2, batch #500, loss: 0.118855044246, 03:39:33\n",
            "\n",
            "evaluating model on: /content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model/data/wl/wl.dev.iob2 \n",
            "\n",
            "sentence head and tail labeling result:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  out-entity   0.993711  0.989089  0.991394     80192\n",
            " head-entity   0.871957  0.886614  0.879224      3272\n",
            " tail-entity   0.821972  0.801208  0.811457      1821\n",
            "   in-entity   0.769895  0.850984  0.808411      3456\n",
            "\n",
            "    accuracy                       0.976076     88741\n",
            "   macro avg   0.864384  0.881974  0.872622     88741\n",
            "weighted avg   0.976981  0.976076  0.976440     88741\n",
            "\n",
            "region classification result:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      neither   0.000000  0.000000  0.000000       777\n",
            "      Disease   0.681159  0.693361  0.687206       949\n",
            "      Finding   0.540856  0.433022  0.480969       963\n",
            "   Medication   0.610390  0.587500  0.598726        80\n",
            "    Procedure   0.619205  0.593651  0.606159       315\n",
            "Family_Member   0.956522  0.758621  0.846154        29\n",
            "    Body_Part   0.730903  0.655763  0.691297       642\n",
            " Abbreviation   0.839378  0.882834  0.860558       734\n",
            "\n",
            "     accuracy                       0.534640      4489\n",
            "    macro avg   0.622302  0.575594  0.596384      4489\n",
            " weighted avg   0.562314  0.534640  0.546708      4489\n",
            "\n",
            " precision = 0.688271, recall = 0.673023, micro_f1 = 0.680561\n",
            "\n",
            "maximum of f1 value: 0.680561, in epoch #2\n",
            "training time: 0:03:11\n",
            "Wed Sep  1 03:39:39 2021\n",
            "\n",
            "epoch #3, batch #0, loss: 0.181433424354, 03:39:39\n",
            "epoch #3, batch #10, loss: 0.244334280491, 03:39:40\n",
            "epoch #3, batch #20, loss: 0.260222434998, 03:39:42\n",
            "epoch #3, batch #30, loss: 0.113946281374, 03:39:43\n",
            "epoch #3, batch #40, loss: 0.161071732640, 03:39:44\n",
            "epoch #3, batch #50, loss: 0.210933566093, 03:39:45\n",
            "epoch #3, batch #60, loss: 0.126561850309, 03:39:46\n",
            "epoch #3, batch #70, loss: 0.207325726748, 03:39:47\n",
            "epoch #3, batch #80, loss: 0.168126493692, 03:39:48\n",
            "epoch #3, batch #90, loss: 0.190107360482, 03:39:49\n",
            "epoch #3, batch #100, loss: 0.131786942482, 03:39:50\n",
            "epoch #3, batch #110, loss: 0.189412564039, 03:39:52\n",
            "epoch #3, batch #120, loss: 0.185803800821, 03:39:53\n",
            "epoch #3, batch #130, loss: 0.198850125074, 03:39:54\n",
            "epoch #3, batch #140, loss: 0.118055932224, 03:39:55\n",
            "epoch #3, batch #150, loss: 0.204892709851, 03:39:56\n",
            "epoch #3, batch #160, loss: 0.184247344732, 03:39:57\n",
            "epoch #3, batch #170, loss: 0.152748137712, 03:39:58\n",
            "epoch #3, batch #180, loss: 0.157829329371, 03:39:59\n",
            "epoch #3, batch #190, loss: 0.077700197697, 03:40:00\n",
            "epoch #3, batch #200, loss: 0.229383736849, 03:40:01\n",
            "epoch #3, batch #210, loss: 0.094617687166, 03:40:03\n",
            "epoch #3, batch #220, loss: 0.155185639858, 03:40:04\n",
            "epoch #3, batch #230, loss: 0.173112645745, 03:40:05\n",
            "epoch #3, batch #240, loss: 0.135035142303, 03:40:06\n",
            "epoch #3, batch #250, loss: 0.105765789747, 03:40:07\n",
            "epoch #3, batch #260, loss: 0.171049028635, 03:40:08\n",
            "epoch #3, batch #270, loss: 0.121048822999, 03:40:10\n",
            "epoch #3, batch #280, loss: 0.132150650024, 03:40:11\n",
            "epoch #3, batch #290, loss: 0.105708524585, 03:40:12\n",
            "epoch #3, batch #300, loss: 0.162156730890, 03:40:13\n",
            "epoch #3, batch #310, loss: 0.167305245996, 03:40:14\n",
            "epoch #3, batch #320, loss: 0.130782186985, 03:40:15\n",
            "epoch #3, batch #330, loss: 0.174068450928, 03:40:16\n",
            "epoch #3, batch #340, loss: 0.173322945833, 03:40:17\n",
            "epoch #3, batch #350, loss: 0.115550741553, 03:40:18\n",
            "epoch #3, batch #360, loss: 0.197735264897, 03:40:19\n",
            "epoch #3, batch #370, loss: 0.129086554050, 03:40:20\n",
            "epoch #3, batch #380, loss: 0.197963729501, 03:40:21\n",
            "epoch #3, batch #390, loss: 0.215499699116, 03:40:23\n",
            "epoch #3, batch #400, loss: 0.181073337793, 03:40:24\n",
            "epoch #3, batch #410, loss: 0.135512053967, 03:40:25\n",
            "epoch #3, batch #420, loss: 0.167075112462, 03:40:26\n",
            "epoch #3, batch #430, loss: 0.143465995789, 03:40:27\n",
            "epoch #3, batch #440, loss: 0.114564776421, 03:40:28\n",
            "epoch #3, batch #450, loss: 0.140949845314, 03:40:29\n",
            "epoch #3, batch #460, loss: 0.128160983324, 03:40:31\n",
            "epoch #3, batch #470, loss: 0.210354581475, 03:40:32\n",
            "epoch #3, batch #480, loss: 0.107199169695, 03:40:33\n",
            "epoch #3, batch #490, loss: 0.112531736493, 03:40:34\n",
            "epoch #3, batch #500, loss: 0.120417028666, 03:40:35\n",
            "\n",
            "evaluating model on: /content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model/data/wl/wl.dev.iob2 \n",
            "\n",
            "sentence head and tail labeling result:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  out-entity   0.993773  0.989076  0.991419     80192\n",
            " head-entity   0.875948  0.882641  0.879281      3272\n",
            " tail-entity   0.811208  0.818781  0.814977      1821\n",
            "   in-entity   0.770894  0.846065  0.806732      3456\n",
            "\n",
            "    accuracy                       0.976088     88741\n",
            "   macro avg   0.862956  0.884141  0.873102     88741\n",
            "weighted avg   0.977002  0.976088  0.976471     88741\n",
            "\n",
            "region classification result:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      neither   0.000000  0.000000  0.000000       879\n",
            "      Disease   0.640367  0.735511  0.684649       949\n",
            "      Finding   0.527913  0.451713  0.486849       963\n",
            "   Medication   0.666667  0.700000  0.682927        80\n",
            "    Procedure   0.623239  0.561905  0.590985       315\n",
            "Family_Member   0.954545  0.724138  0.823529        29\n",
            "    Body_Part   0.769091  0.658879  0.709732       642\n",
            " Abbreviation   0.835687  0.886921  0.860542       734\n",
            "\n",
            "     accuracy                       0.536049      4591\n",
            "    macro avg   0.627189  0.589883  0.604902      4591\n",
            " weighted avg   0.544669  0.536049  0.538125      4591\n",
            "\n",
            " precision = 0.677402, recall = 0.690129, micro_f1 = 0.683706\n",
            "\n",
            "maximum of f1 value: 0.683706, in epoch #3\n",
            "training time: 0:04:13\n",
            "Wed Sep  1 03:40:41 2021\n",
            "\n",
            "epoch #4, batch #0, loss: 0.169231399894, 03:40:41\n",
            "epoch #4, batch #10, loss: 0.166979908943, 03:40:43\n",
            "epoch #4, batch #20, loss: 0.227884471416, 03:40:44\n",
            "epoch #4, batch #30, loss: 0.115968115628, 03:40:45\n",
            "epoch #4, batch #40, loss: 0.180120527744, 03:40:46\n",
            "epoch #4, batch #50, loss: 0.188795447350, 03:40:47\n",
            "epoch #4, batch #60, loss: 0.113031178713, 03:40:48\n",
            "epoch #4, batch #70, loss: 0.191396653652, 03:40:50\n",
            "epoch #4, batch #80, loss: 0.172125041485, 03:40:51\n",
            "epoch #4, batch #90, loss: 0.188525363803, 03:40:52\n",
            "epoch #4, batch #100, loss: 0.119141176343, 03:40:53\n",
            "epoch #4, batch #110, loss: 0.175816506147, 03:40:54\n",
            "epoch #4, batch #120, loss: 0.161389529705, 03:40:55\n",
            "epoch #4, batch #130, loss: 0.185846269131, 03:40:56\n",
            "epoch #4, batch #140, loss: 0.122092798352, 03:40:57\n",
            "epoch #4, batch #150, loss: 0.143802180886, 03:40:58\n",
            "epoch #4, batch #160, loss: 0.167460888624, 03:41:00\n",
            "epoch #4, batch #170, loss: 0.129259675741, 03:41:01\n",
            "epoch #4, batch #180, loss: 0.109109953046, 03:41:02\n",
            "epoch #4, batch #190, loss: 0.068765446544, 03:41:03\n",
            "epoch #4, batch #200, loss: 0.169935226440, 03:41:04\n",
            "epoch #4, batch #210, loss: 0.059368528426, 03:41:05\n",
            "epoch #4, batch #220, loss: 0.120097622275, 03:41:07\n",
            "epoch #4, batch #230, loss: 0.119603857398, 03:41:08\n",
            "epoch #4, batch #240, loss: 0.113046795130, 03:41:09\n",
            "epoch #4, batch #250, loss: 0.113824918866, 03:41:10\n",
            "epoch #4, batch #260, loss: 0.147037774324, 03:41:11\n",
            "epoch #4, batch #270, loss: 0.110064119101, 03:41:12\n",
            "epoch #4, batch #280, loss: 0.121545910835, 03:41:13\n",
            "epoch #4, batch #290, loss: 0.091863304377, 03:41:14\n",
            "epoch #4, batch #300, loss: 0.138031840324, 03:41:15\n",
            "epoch #4, batch #310, loss: 0.143033340573, 03:41:17\n",
            "epoch #4, batch #320, loss: 0.098108500242, 03:41:18\n",
            "epoch #4, batch #330, loss: 0.144195854664, 03:41:19\n",
            "epoch #4, batch #340, loss: 0.152596592903, 03:41:20\n",
            "epoch #4, batch #350, loss: 0.097968071699, 03:41:21\n",
            "epoch #4, batch #360, loss: 0.141630798578, 03:41:22\n",
            "epoch #4, batch #370, loss: 0.097481220961, 03:41:23\n",
            "epoch #4, batch #380, loss: 0.160993635654, 03:41:24\n",
            "epoch #4, batch #390, loss: 0.212177544832, 03:41:25\n",
            "epoch #4, batch #400, loss: 0.156582012773, 03:41:27\n",
            "epoch #4, batch #410, loss: 0.112553633749, 03:41:28\n",
            "epoch #4, batch #420, loss: 0.139888912439, 03:41:29\n",
            "epoch #4, batch #430, loss: 0.122371576726, 03:41:30\n",
            "epoch #4, batch #440, loss: 0.091787956655, 03:41:31\n",
            "epoch #4, batch #450, loss: 0.104110702872, 03:41:32\n",
            "epoch #4, batch #460, loss: 0.121642097831, 03:41:33\n",
            "epoch #4, batch #470, loss: 0.145465716720, 03:41:34\n",
            "epoch #4, batch #480, loss: 0.088180229068, 03:41:36\n",
            "epoch #4, batch #490, loss: 0.111198037863, 03:41:37\n",
            "epoch #4, batch #500, loss: 0.101973325014, 03:41:38\n",
            "\n",
            "evaluating model on: /content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model/data/wl/wl.dev.iob2 \n",
            "\n",
            "sentence head and tail labeling result:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  out-entity   0.992355  0.990610  0.991482     80192\n",
            " head-entity   0.893013  0.875000  0.883915      3272\n",
            " tail-entity   0.825549  0.805601  0.815453      1821\n",
            "   in-entity   0.784462  0.841435  0.811950      3456\n",
            "\n",
            "    accuracy                       0.976741     88741\n",
            "   macro avg   0.873845  0.878162  0.875700     88741\n",
            "weighted avg   0.977173  0.976741  0.976912     88741\n",
            "\n",
            "region classification result:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      neither   0.000000  0.000000  0.000000       798\n",
            "      Disease   0.698603  0.737619  0.717581       949\n",
            "      Finding   0.549296  0.485981  0.515702       963\n",
            "   Medication   0.787879  0.650000  0.712329        80\n",
            "    Procedure   0.656028  0.587302  0.619765       315\n",
            "Family_Member   0.880000  0.758621  0.814815        29\n",
            "    Body_Part   0.766071  0.668224  0.713810       642\n",
            " Abbreviation   0.825758  0.891008  0.857143       734\n",
            "\n",
            "     accuracy                       0.556541      4510\n",
            "    macro avg   0.645454  0.597344  0.618893      4510\n",
            " weighted avg   0.573186  0.556541  0.563383      4510\n",
            "\n",
            " precision = 0.701313, recall = 0.703870, micro_f1 = 0.702589\n",
            "\n",
            "maximum of f1 value: 0.702589, in epoch #4\n",
            "training time: 0:05:15\n",
            "Wed Sep  1 03:41:44 2021\n",
            "\n",
            "epoch #5, batch #0, loss: 0.150258526206, 03:41:44\n",
            "epoch #5, batch #10, loss: 0.154563248158, 03:41:45\n",
            "epoch #5, batch #20, loss: 0.219745665789, 03:41:46\n",
            "epoch #5, batch #30, loss: 0.094474911690, 03:41:47\n",
            "epoch #5, batch #40, loss: 0.134279072285, 03:41:48\n",
            "epoch #5, batch #50, loss: 0.134316921234, 03:41:50\n",
            "epoch #5, batch #60, loss: 0.098548144102, 03:41:51\n",
            "epoch #5, batch #70, loss: 0.166131258011, 03:41:52\n",
            "epoch #5, batch #80, loss: 0.129467129707, 03:41:53\n",
            "epoch #5, batch #90, loss: 0.162418320775, 03:41:54\n",
            "epoch #5, batch #100, loss: 0.092132955790, 03:41:55\n",
            "epoch #5, batch #110, loss: 0.146842718124, 03:41:57\n",
            "epoch #5, batch #120, loss: 0.156457394361, 03:41:58\n",
            "epoch #5, batch #130, loss: 0.156596213579, 03:41:59\n",
            "epoch #5, batch #140, loss: 0.082090333104, 03:42:00\n",
            "epoch #5, batch #150, loss: 0.123059883714, 03:42:01\n",
            "epoch #5, batch #160, loss: 0.160404726863, 03:42:02\n",
            "epoch #5, batch #170, loss: 0.113815769553, 03:42:03\n",
            "epoch #5, batch #180, loss: 0.133377492428, 03:42:04\n",
            "epoch #5, batch #190, loss: 0.043975628912, 03:42:05\n",
            "epoch #5, batch #200, loss: 0.162392973900, 03:42:06\n",
            "epoch #5, batch #210, loss: 0.050618436188, 03:42:08\n",
            "epoch #5, batch #220, loss: 0.139458432794, 03:42:09\n",
            "epoch #5, batch #230, loss: 0.128892779350, 03:42:10\n",
            "epoch #5, batch #240, loss: 0.102945663035, 03:42:11\n",
            "epoch #5, batch #250, loss: 0.096155099571, 03:42:12\n",
            "epoch #5, batch #260, loss: 0.132381588221, 03:42:13\n",
            "epoch #5, batch #270, loss: 0.107304938138, 03:42:14\n",
            "epoch #5, batch #280, loss: 0.108627580106, 03:42:15\n",
            "epoch #5, batch #290, loss: 0.079796217382, 03:42:17\n",
            "epoch #5, batch #300, loss: 0.126533448696, 03:42:18\n",
            "epoch #5, batch #310, loss: 0.116877242923, 03:42:19\n",
            "epoch #5, batch #320, loss: 0.084542542696, 03:42:20\n",
            "epoch #5, batch #330, loss: 0.146310120821, 03:42:21\n",
            "epoch #5, batch #340, loss: 0.139760658145, 03:42:22\n",
            "epoch #5, batch #350, loss: 0.086999043822, 03:42:23\n",
            "epoch #5, batch #360, loss: 0.161600723863, 03:42:24\n",
            "epoch #5, batch #370, loss: 0.102958053350, 03:42:25\n",
            "epoch #5, batch #380, loss: 0.158444032073, 03:42:26\n",
            "epoch #5, batch #390, loss: 0.164824992418, 03:42:28\n",
            "epoch #5, batch #400, loss: 0.144814148545, 03:42:29\n",
            "epoch #5, batch #410, loss: 0.101313926280, 03:42:30\n",
            "epoch #5, batch #420, loss: 0.149427175522, 03:42:31\n",
            "epoch #5, batch #430, loss: 0.110129937530, 03:42:32\n",
            "epoch #5, batch #440, loss: 0.083079382777, 03:42:34\n",
            "epoch #5, batch #450, loss: 0.114893235266, 03:42:35\n",
            "epoch #5, batch #460, loss: 0.113865852356, 03:42:36\n",
            "epoch #5, batch #470, loss: 0.126132220030, 03:42:37\n",
            "epoch #5, batch #480, loss: 0.088848426938, 03:42:38\n",
            "epoch #5, batch #490, loss: 0.114314496517, 03:42:39\n",
            "epoch #5, batch #500, loss: 0.095806658268, 03:42:40\n",
            "\n",
            "evaluating model on: /content/gdrive/My Drive/mlc-paper-baselines/boundary-aware-nested-ner/Our_boundary-aware_model/data/wl/wl.dev.iob2 \n",
            "\n",
            "sentence head and tail labeling result:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  out-entity   0.992383  0.989388  0.990883     80192\n",
            " head-entity   0.893805  0.864303  0.878807      3272\n",
            " tail-entity   0.820321  0.784734  0.802133      1821\n",
            "   in-entity   0.752124  0.845486  0.796077      3456\n",
            "\n",
            "    accuracy                       0.974972     88741\n",
            "   macro avg   0.864658  0.870978  0.866975     88741\n",
            "weighted avg   0.975860  0.974972  0.975291     88741\n",
            "\n",
            "region classification result:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      neither   0.000000  0.000000  0.000000       766\n",
            "      Disease   0.708595  0.712329  0.710457       949\n",
            "      Finding   0.553325  0.457944  0.501136       963\n",
            "   Medication   0.860000  0.537500  0.661538        80\n",
            "    Procedure   0.661538  0.546032  0.598261       315\n",
            "Family_Member   0.958333  0.793103  0.867925        29\n",
            "    Body_Part   0.750000  0.663551  0.704132       642\n",
            " Abbreviation   0.824051  0.886921  0.854331       734\n",
            "\n",
            "     accuracy                       0.543100      4478\n",
            "    macro avg   0.664480  0.574673  0.612223      4478\n",
            " weighted avg   0.579866  0.543100  0.558842      4478\n",
            "\n",
            " precision = 0.706361, recall = 0.681997, micro_f1 = 0.693965\n",
            "\n",
            "maximum of f1 value: 0.702589, in epoch #4\n",
            "training time: 0:06:17\n",
            "Wed Sep  1 03:42:46 2021\n",
            "\n",
            "epoch #6, batch #0, loss: 0.165048971772, 03:42:46\n",
            "epoch #6, batch #10, loss: 0.112778939307, 03:42:47\n",
            "epoch #6, batch #20, loss: 0.196748197079, 03:42:48\n",
            "epoch #6, batch #30, loss: 0.074703820050, 03:42:49\n",
            "epoch #6, batch #40, loss: 0.134124204516, 03:42:51\n",
            "epoch #6, batch #50, loss: 0.133511692286, 03:42:52\n",
            "epoch #6, batch #60, loss: 0.083850070834, 03:42:53\n",
            "epoch #6, batch #70, loss: 0.179002463818, 03:42:54\n",
            "epoch #6, batch #80, loss: 0.095991954207, 03:42:55\n",
            "epoch #6, batch #90, loss: 0.161017209291, 03:42:56\n",
            "epoch #6, batch #100, loss: 0.104506336153, 03:42:57\n",
            "epoch #6, batch #110, loss: 0.118354231119, 03:42:59\n",
            "epoch #6, batch #120, loss: 0.136308088899, 03:43:00\n",
            "epoch #6, batch #130, loss: 0.144989624619, 03:43:01\n",
            "epoch #6, batch #140, loss: 0.067508883774, 03:43:02\n",
            "epoch #6, batch #150, loss: 0.102214187384, 03:43:03\n",
            "epoch #6, batch #160, loss: 0.140793114901, 03:43:04\n",
            "epoch #6, batch #170, loss: 0.094971120358, 03:43:05\n",
            "epoch #6, batch #180, loss: 0.108348786831, 03:43:06\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 180, in <module>\n",
            "    main()\n",
            "  File \"train.py\", line 174, in main\n",
            "    train_end2end(test_url=TEST_URL)\n",
            "  File \"train.py\", line 126, in train_end2end\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 255, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 149, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gEl-SyIcxbf"
      },
      "source": [
        "Replace eval.py with the one we passed in the supplementary material. This edited file writes the result of the predictions in a text file, which can be used to calculate task-specific metrics. The name of the file is boundary_result.txt. Also, change the following lines, according to your dataset (just in testing step). In the \"model\" folder you will find the best model save.\n",
        "\n",
        "```\n",
        "model_url = from_project_root(\"data/model/end2end_model_epochnumber_score.pt\")\n",
        "test_url = from_project_root(\"data/wl/wl.test.iob2\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nFyH0vfQdYD"
      },
      "source": [
        "!python eval.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4NzINPiNc19"
      },
      "source": [
        "# ***Second-best model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2K9Ldn1WoFD",
        "outputId": "3622a7b4-90a8-43fc-f00c-70e81bcb35e1"
      },
      "source": [
        "%cd mlc-baselines/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/mlc-baselines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zFIZr9yNHvj",
        "outputId": "aafd7293-0744-4135-c991-80bb8dc09505"
      },
      "source": [
        "# Clone the project from the unofficial repository, if you have already cloned it skip this step.\n",
        "!git clone https://github.com/yahshibu/nested-ner-tacl2020-flair.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nested-ner-tacl2020-flair'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 45 (delta 9), reused 45 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (45/45), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLTpyE52XnOc",
        "outputId": "ddb6cf04-f6a2-4a25-e4ca-9435d28c81e9"
      },
      "source": [
        "%cd nested-ner-tacl2020-flair/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/mlc-baselines/nested-ner-tacl2020-flair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyMf5YbDB83i"
      },
      "source": [
        "!mkdir embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX9mbOt_OUjk"
      },
      "source": [
        "Place the embeddings described in the repository in the embeddings folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rKxC3UVDipZ",
        "outputId": "8a722eb9-21cc-4f2d-d602-886f59ff78cc"
      },
      "source": [
        "!pip install adabound\n",
        "!pip install flair"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: adabound in /usr/local/lib/python3.7/dist-packages (0.0.5)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from adabound) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->adabound) (3.7.4.3)\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.7/dist-packages (0.9)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from flair) (0.0.16)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.7/dist-packages (from flair) (1.2.12)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n",
            "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: sentencepiece==0.1.95 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.95)\n",
            "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.6.5)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (from flair) (1.0.9)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.7.0)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.3)\n",
            "Requirement already satisfied: more-itertools~=8.8.0 in /usr/local/lib/python3.7/dist-packages (from flair) (8.8.0)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.9.0+cu102)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from flair) (0.3.3)\n",
            "Requirement already satisfied: janome in /usr/local/lib/python3.7/dist-packages (from flair) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.62.0)\n",
            "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.10.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: gdown==3.12.2 in /usr/local/lib/python3.7/dist-packages (from flair) (3.12.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.7/dist-packages (from flair) (0.5.4)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from flair) (6.0.3)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.7/dist-packages (from flair) (1.5.10)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.2)\n",
            "Requirement already satisfied: conllu>=4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (2.26.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair) (1.19.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.6.2)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: overrides<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from konoha<5.0.0,>=4.0.0->flair) (3.1.0)\n",
            "Requirement already satisfied: importlib-metadata<4.0.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konoha<5.0.0,>=4.0.0->flair) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.5.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (2021.5.30)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (5.4.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYR_bPUSOWuH"
      },
      "source": [
        "Put in the data/wl folder the three files wl.train, wl.dev, wl.test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRnYhffDOmEh"
      },
      "source": [
        "Add to the folder the files gen_data_for_wl.py and gen_data_for_germ.py\n",
        "\n",
        "Change the reader file to the one we deliver.\n",
        "\n",
        "Change the embeddings contextualized in the sequence_labeling.py file to those described in the repository\n",
        "\n",
        "Change the hyperparameters in the configuration file and links."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAW7_cfsDGFb",
        "outputId": "865487cf-e405-4514-cfb1-619bf20a6341"
      },
      "source": [
        "!python gen_data_for_wl.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length: 42\n",
            "Threshold 6: 2734\n",
            "Max length: 35\n",
            "Threshold 6: 277\n",
            "Max length: 29\n",
            "Threshold 6: 397\n",
            "# mentions: 35480\n",
            "# mentions: 3971\n",
            "# mentions: 4289\n",
            "All 35480, start 0, end 0\n",
            "All 3971, start 0, end 0\n",
            "All 4289, start 0, end 0\n",
            "Remember to scp word vectors as well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmYAvvaPQfJE"
      },
      "source": [
        "Module - crf - change device problem: indices_3 = indices_3.cuda() before \"while t greater than -1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a88UQ-TBRkbA"
      },
      "source": [
        "Add W2Vebedding class to sequence labeling...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdkM5GjyGHYX",
        "outputId": "83623514-80b4-4c5f-ec45-f4f0a0a798bd"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-08-31 22:58:24,684 - Nested Mention - INFO - {'root_path': '.', 'data_set': 'wl', 'batch_size': 8, 'if_shuffle': True, 'label_size': 7, 'char_embed': 32, 'num_filters': 32, 'word_dropout': 0.3, 'hidden_size': 256, 'layers': 2, 'lstm_dropout': 0.5, 'embed_path': './data/word_vec_wl.vec', 'epoch': 1, 'if_gpu': True, 'opt': <Optimizer.SGD: 'SGD'>, 'lr': 0.1, 'final_lr': None, 'l2': 0.0, 'check_every': 1, 'clip_norm': 5, 'lr_patience': 5, 'data_path': './data/wl', 'train_data_path': './data/wl_train.pkl', 'dev_data_path': './data/wl_dev.pkl', 'test_data_path': './data/wl_test.pkl', 'config_data_path': './data/wl_config.pkl', 'model_root_path': './dumps', 'model_path': './dumps/wl_model'}\n",
            "2021-08-31 22:58:45,267 - Nested Mention - INFO - 251 batches expected for training\n",
            "2021-08-31 22:58:45,267 - Nested Mention - INFO - \n",
            "2021-08-31 22:58:45,268 - Nested Mention - INFO - Epoch 1 (learning rate=0.1000):\n",
            "/content/gdrive/MyDrive/mlc-baselines/nested-ner-tacl2020-flair/model/sequence_labeling.py:45: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  word_embedding = torch.FloatTensor(word_embedding)\n",
            "train: 10/251 loss: 74.3581, time left (estimated): 218.39s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 20/251 loss: 49.2064, time left (estimated): 181.83s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 30/251 loss: 41.2594, time left (estimated): 169.17s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 40/251 loss: 36.7284, time left (estimated): 158.20s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 50/251 loss: 34.7505, time left (estimated): 153.65s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 60/251 loss: 35.2192, time left (estimated): 158.02s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 70/251 loss: 36.1845, time left (estimated): 160.54s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 80/251 loss: 35.2476, time left (estimated): 152.35s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 90/251 loss: 33.3486, time left (estimated): 138.92s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 100/251 loss: 31.5699, time left (estimated): 125.88s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 110/251 loss: 30.4964, time left (estimated): 115.80s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 120/251 loss: 30.2847, time left (estimated): 109.35s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 130/251 loss: 29.3046, time left (estimated): 99.00s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 140/251 loss: 28.7056, time left (estimated): 90.28s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 150/251 loss: 28.4619, time left (estimated): 82.22s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 160/251 loss: 28.2924, time left (estimated): 74.53s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 170/251 loss: 28.2240, time left (estimated): 67.16s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 180/251 loss: 27.9873, time left (estimated): 59.12s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 190/251 loss: 27.5242, time left (estimated): 50.45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 200/251 loss: 27.4643, time left (estimated): 42.56s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 210/251 loss: 27.2366, time left (estimated): 34.20s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 220/251 loss: 26.7583, time left (estimated): 25.59s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 230/251 loss: 26.8685, time left (estimated): 17.58s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 240/251 loss: 26.9476, time left (estimated): 9.31s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 250/251 loss: 27.0070, time left (estimated): 0.86s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2021-08-31 23:02:19,544 - Nested Mention - INFO - train: 251 loss: 26.9311, time: 214.28s\n"
          ]
        }
      ]
    }
  ]
}