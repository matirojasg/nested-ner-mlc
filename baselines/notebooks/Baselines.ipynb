{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Baselines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PthZlIj6MBdk"
      },
      "source": [
        "# ***ACL22 MLC Paper - Baselines***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVp9qKLaISXT"
      },
      "source": [
        "This repository contains all the instructions to reproduce the state-of-the-art models that we use as baselines in our paper. We remind you that these repositories are maintained by the authors, so if any problem arises with their code, please add an issue in the corresponding repositories.\n",
        "\n",
        "Source codes:\n",
        "\n",
        "1. Layered: [https://github.com/meizhiju/layered-bilstm-crf](https://github.com/meizhiju/layered-bilstm-crf)\n",
        "2. Exhaustive: [https://github.com/csJd/deep_exhaustive_model](https://github.com/csJd/deep_exhaustive_model)\n",
        "3. Boundary: [https://github.com/thecharm/boundary-aware-nested-ner](https://github.com/thecharm/boundary-aware-nested-ner)\n",
        "4. Recursive-CRF: [https://github.com/yahshibu/nested-ner-tacl2020-flair](https://github.com/yahshibu/nested-ner-tacl2020-flair)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2GikXE2MA9N"
      },
      "source": [
        "# First, we set up the working environment in google drive. If you are working locally, it will not be necessary but make sure that you are using the GPU.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMDH8nP_3Jew"
      },
      "source": [
        "# We will clone the repositories in the \"MyDrive\" folder.\n",
        "%cd gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wGhxSnUMzZi"
      },
      "source": [
        "# We create a folder where we will clone each repository. If the folder is already created, then skip this step.\n",
        "!mkdir mlc-paper-baselines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3O6psnaMn9x"
      },
      "source": [
        "# ***Layered model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t_ab80Qun4S"
      },
      "source": [
        "# We advance to the folder where we will save the baselines.\n",
        "%cd mlc-paper-baselines/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eoNNFiHNTff"
      },
      "source": [
        "# Clone the project from the official repository. If you have already cloned it, skip this step.\n",
        "!git clone https://github.com/meizhiju/layered-bilstm-crf.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jUVwJwRNtb_"
      },
      "source": [
        "# Navigate to the \"src\" folder. This folder contains the main scripts with which we will train and test this model.\n",
        "%cd layered-bilstm-crf/src/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP_iU3JOOFpl"
      },
      "source": [
        "On the left side of Google Colab, you can navigate to the folder containing the project (MyDrive > mlc-paper-baselines > layered-bilstm-crf > src). To execute this model, we had to make the following changes to the source code:\n",
        "\n",
        "1. **Data loading**: In the folder named \"dataset\", put the files train.data, dev.data and test.data. Make sure that you follow the format explained in the repository. \n",
        "\n",
        "2. **Embeddings**: Create a folder named \"embeddings\" inside the src folder, and put the embeddings described in the repository.\n",
        "\n",
        "3. **Hyperparameters**: Change the hyperparameters and directories in the config file (add: ../src/embeddings/embedding_name to the path_pre_emb param). If you are working with the Chilean Waiting List corpus, remember that these clinical embeddings have a dimension of 300. If you are working with a GPU, set the 'main' key to 0, and if you are training, set the mode key to 'train'. As an example in the Chilean corpus:\n",
        "\n",
        "\n",
        "```\n",
        "- word_embedding_dim: 300 \n",
        "- char_embedding_dim: 25\n",
        "- dropout_ratio: 0.3\n",
        "- lr_param: 0.001\n",
        "- threshold: 5 \n",
        "- decay_rate: 0\n",
        "- batch_size: 16\n",
        "- tags: 7 \n",
        "- epochs: 20\n",
        "- replace_digit: false\n",
        "- lowercase: false\n",
        "- use_singletons: false\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4. **Code fixes**: Go to the layered_model.py script in the model folder. Due to an error generated by the class called \"Evaluator\", you must make the following two changes in that class:\n",
        "\n",
        "```\n",
        "1. Before (line 397): def __init__(self, iterator, target, device): \n",
        "   Now: def __init__(self, iterator, target, device=cuda.cupy):\n",
        "\n",
        "2. Before (line 398): super(Evaluator, self).__init__(iterator=iterator, target=target, device=device) \n",
        "   Now: super(Evaluator, self).__init__(iterator=iterator, target=target)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBDXeKjLRf78"
      },
      "source": [
        "# We install the repository dependencies.\n",
        "!pip install chainer\n",
        "!pip install texttable\n",
        "!pip install 'cupy-cuda101>=7.7.0,<8.0.0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmUI11j0Rc6Z"
      },
      "source": [
        "# Next, we train the Layered model.\n",
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms_IG9ynTWkM"
      },
      "source": [
        "To evaluate, change the training mode to test in the configuration file and specify the path to the best model located in the \"result\" folder using the \"path_model\" key. In the \"evaluation\" folder will be found the file with the output. Then, we can calculate the task-specific metrics using this file (go to the metrics Jupyter notebook). Also, in utils.py in the \"model\" folder, you have to comment on the following lines to save the predictions file:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "1. Before (line 202): os.remove(output_path)\n",
        "   Now: #os.remove(output_path)\n",
        "\n",
        "2. Before (line 203): os.remove(scores_path)\n",
        "   Now: #os.remove(scores_path)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7tKBXqxTkyC"
      },
      "source": [
        "# And we run the best model calculated on the validation set, now on our testing data. The file with the predictions will be located in the \"evaluation\" folder with the extension '.scores'.\n",
        "!python test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOydLqBrNT9r"
      },
      "source": [
        "# ***Exhaustive model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itk8XpVJTARJ"
      },
      "source": [
        "# If you were in the layered folder\n",
        "#%cd ..\n",
        "#%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ymfuYj_S9Fy"
      },
      "source": [
        "# We advance to the folder where we will save the baselines.\n",
        "%cd mlc-paper-baselines/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q95tho4q67Nt"
      },
      "source": [
        "# Clone the project from the official repository. If you have already cloned it, skip this step.\n",
        "!git clone https://github.com/csJd/deep_exhaustive_model.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5PwoDJhTSXM"
      },
      "source": [
        "# In the folder called \"data\", we will create a folder for each dataset. We also create a folder for the models.\n",
        "%cd deep_exhaustive_model/data\n",
        "!mkdir genia\n",
        "!mkdir wl\n",
        "!mkdir germ\n",
        "!mkdir model\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZoSyOqPpbfl"
      },
      "source": [
        "On the left side of Google Colab, you can navigate to the folder containing the project (MyDrive > mlc-paper-baselines > deep_exhaustive_model). To execute this model, we had to make the following changes to the source code:\n",
        "\n",
        "For example using the Chilean Waiting List dataset.\n",
        "\n",
        "1. **Data loading**: In the folder named \"data\", wl.put the files train.iob2, wl.dev.iob2 and wl.test.iob2 in the wl folder, following the format explained in the repository (As an example, we will use the case of the waiting list).\n",
        "\n",
        "2. **Embeddings**: Put the embeddings described in the repository in the \"embedding\" folder, which is inside the \"data\" folder.\n",
        "\n",
        "3. **Hyperparameters**: Change the hyperparameters and directories in the \"train.py\" script. If you are working with the Chilean Waiting List corpus, remember that these clinical embeddings have a dimension of 300 (line 90). As an example in the Chilean corpus:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "- embedding_dim: 300 \n",
        "- char_feat_dim: 25\n",
        "- learning_rate: 0.001\n",
        "- clip_norm: 5 \n",
        "- batch_size: 16\n",
        "- epochs: 30\n",
        "- hidden_size = 128\n",
        "```\n",
        "\n",
        "4. **Code fixes**: You must make the following changes in lines 24 to 28.\n",
        "\n",
        "```\n",
        "EMBD_URL = from_project_root(\"data/embedding/cwlce.vec\")\n",
        "VOCAB_URL = from_project_root(\"data/wl/vocab.json\")\n",
        "TRAIN_URL = from_project_root(\"data/wl/wl.train.iob2\")\n",
        "DEV_URL = from_project_root(\"data/wl/wl.dev.iob2\")\n",
        "TEST_URL = from_project_root(\"data/wl/wl.test.iob2\")\n",
        "```\n",
        "\n",
        "In the \"dataset.py script\", you have to change \"data_urls\" (line 272) variable to: \n",
        "\n",
        "```\n",
        "data_urls = [from_project_root(\"data/wl/wl.train.iob2\"),\n",
        "                 from_project_root(\"data/wl/wl.dev.iob2\"),\n",
        "                 from_project_root(\"data/wl/wl.test.iob2\")]\n",
        "```\n",
        "\n",
        "Add the pre-trained embedding path (line 275)\n",
        "```\n",
        "prepare_vocab(data_urls, from_project_root(\"data/embedding/cwlce.vec\"), update=True, min_count=1)\n",
        "```\n",
        "\n",
        "Finally, in \"model.py\", change the CharLSTM class (line 101), use the following:\n",
        "\n",
        "\n",
        "```\n",
        "class CharLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_chars, embedding_size, hidden_size, lstm_layers=1, bidirectional=True):\n",
        "        super().__init__()\n",
        "        self.n_chars = n_chars\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_hidden = hidden_size * (1 + bidirectional)\n",
        "\n",
        "        self.embedding = nn.Embedding(n_chars, embedding_size, padding_idx=0)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_size,\n",
        "            hidden_size=hidden_size,\n",
        "            bidirectional=bidirectional,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def sent_forward(self, words, lengths, indices):\n",
        "        sent_len = words.shape[0]\n",
        "        # words shape: (sent_len, max_word_len)\n",
        "\n",
        "        embedded = self.embedding(words)\n",
        "        # in_data shape: (sent_len, max_word_len, embedding_dim)\n",
        "\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu().numpy(), batch_first=True)\n",
        "        _, (hn, _) = self.lstm(packed)\n",
        "        # shape of hn:  (n_layers * n_directions, sent_len, hidden_size)\n",
        "\n",
        "        hn = hn.permute(1, 0, 2).contiguous().view(sent_len, -1)\n",
        "        hn2 = hn.clone()\n",
        "        # shape of hn:  (sent_len, n_layers * n_directions * hidden_size) = (sent_len, 2*hidden_size)\n",
        "\n",
        "        # shape of indices: (sent_len, max_word_len)\n",
        "        hn[indices] = hn2  # unsort hn\n",
        "        # unsorted = hn.new_empty(hn.size())\n",
        "        # unsorted.scatter_(dim=0, index=indices.unsqueeze(-1).expand_as(hn), src=hn)\n",
        "        return hn\n",
        "\n",
        "    def forward(self, sentence_words, sentence_word_lengths, sentence_word_indices):\n",
        "        # sentence_words [batch_size, *sent_len, max_word_len]\n",
        "        # sentence_word_lengths [batch_size, *sent_len]\n",
        "        # sentence_word_indices [batch_size, *sent_len, max_word_len]\n",
        "\n",
        "        batch_size = len(sentence_words)\n",
        "        batch_char_feat = torch.nn.utils.rnn.pad_sequence(\n",
        "            [self.sent_forward(sentence_words[i], sentence_word_lengths[i], sentence_word_indices[i])\n",
        "             for i in range(batch_size)], batch_first=True)\n",
        "\n",
        "        return batch_char_feat\n",
        "        # (batch_size, sent_len, 2 * hidden_size)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgT-Hr6Prvgj"
      },
      "source": [
        "# Now, we create the dataset pickle files.\n",
        "!python dataset.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGrZtJwntoAz"
      },
      "source": [
        "# And we train the Exhaustive model.\n",
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdq_VkBQwKW3"
      },
      "source": [
        "To evaluate and generate the prediction file, in the eval.py script, uncomment the following line (line 144): \n",
        "\n",
        "```\n",
        "# predict_on_iob2(model, test_url)\n",
        "```\n",
        "\n",
        "Change the model_url to the best model found (line 139):\n",
        "\n",
        "```\n",
        "model_url = from_project_root(\"data/model/model.pt\")\n",
        "```\n",
        "\n",
        "And, change the test_url (line 143):\n",
        "\n",
        "\n",
        "```\n",
        "test_url = from_project_root(\"data/wl/wl.test.iob2\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU2CONfAws4O"
      },
      "source": [
        "!python eval.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfIqwN2TwzEO"
      },
      "source": [
        "In the corpus folder (wl in this example) will be located the wl.test.pred.txt file with the output, to which the metrics can be calculated in our notebook of task-specific metrics. Note that in this output, the multilabel entities are not considered. Therefore, the Chilean Waiting List predictions must be compared with the original data to obtain the objective metric. This is done in the metrics notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzeZ-TZmNX1u"
      },
      "source": [
        "# ***Boundary model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBfRj-RXXiNc"
      },
      "source": [
        "# If you were in the exhaustive folder\n",
        "# %cd ..\n",
        "# %cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvgpBj1yT3Fu"
      },
      "source": [
        "# Clone the project from the official repository. If you have already cloned it, skip this step.\n",
        "%cd mlc-paper-baselines/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0-1AtW3Naos"
      },
      "source": [
        "# Clone the project from the unofficial repository, if you have already cloned it skip this step.\n",
        "!git clone https://github.com/thecharm/boundary-aware-nested-ner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZPHldiwvHyh"
      },
      "source": [
        "%cd boundary-aware-nested-ner/Our_boundary-aware_model/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VFHMZgN211N"
      },
      "source": [
        "# In the folder called \"data\", we will create a folder for each dataset. We also create a folder for the models and the embeddings folder.\n",
        "%cd data\n",
        "!mkdir wl\n",
        "!mkdir germ\n",
        "!mkdir model\n",
        "!mkdir embedding\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1LeIlxLy81_"
      },
      "source": [
        "On the left side of Google Colab, you can navigate to the folder containing the project (MyDrive > mlc-paper-baselines > boundary-aware-nested-ner > Our_boundary-aware_model). To execute this model, we had to make the following changes to the source code:\n",
        "\n",
        "For example using the Chilean Waiting List dataset.\n",
        "\n",
        "1. **Data loading**: In the \"data\" folder, place the files wl.train.iob2, wl.dev.iob2 and wl.test.iob2 in the wl folder, following the format explained in the repository. (As an example, we will use the case of the waiting list).\n",
        "\n",
        "2. **Embeddings**: Put the embeddings described in the repository in the \"embedding\" folder, which is inside the \"data\" folder.\n",
        "\n",
        "3. **Hyperparameters**: Change the hyperparameters and directories in the \"train.py\" script. If you are working with the Chilean Waiting List corpus, remember that these clinical embeddings have a dimension of 300 (line 95). \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "MAX_REGION = 10\n",
        "EARLY_STOP = 5\n",
        "LR = 0.005\n",
        "BATCH_SIZE = 16 \n",
        "MAX_GRAD_NORM = 5\n",
        "N_TAGS = 8\n",
        "TAG_WEIGHTS = [1, 1, 1, 1, 1, 1, 1, 1]\n",
        "FREEZE_WV = False\n",
        "LOG_PER_BATCH = 10\n",
        "hidden_size=128,\n",
        "lstm_layers=3\n",
        "```\n",
        "\n",
        "\n",
        "4. **Code fixes**\n",
        "\n",
        "Lines 34-38\n",
        "\n",
        "```\n",
        "PRETRAINED_URL = from_project_root(\"data/embedding/cwlce.vec\")\n",
        "EMBED_URL = from_project_root(\"data/wl/embeddings.npy\")\n",
        "VOCAB_URL = from_project_root(\"data/wl/vocab.json\")\n",
        "TRAIN_URL = from_project_root(\"data/wl/wl.train.iob2\")\n",
        "DEV_URL = from_project_root(\"data/wl/wl.dev.iob2\")\n",
        "TEST_URL = from_project_root(\"data/wl/wl.test.iob2\")\n",
        "```\n",
        "\n",
        "Change tags and weights\n",
        "```\n",
        "N_TAGS = 8 (line 29)\n",
        "TAG_WEIGHTS = [1, 1, 1, 1, 1, 1, 1, 1] (line 30)\n",
        "```\n",
        "\n",
        "And delete the following lines:\n",
        "```\n",
        "import pdb (line 21)\n",
        "pdb.set_trace() (line 22)\n",
        "```\n",
        "\n",
        "In the \"dataset.py\" script, change data_urls (line 286) to \n",
        "\n",
        "```\n",
        "data_urls = [from_project_root(\"data/wl/wl.train.iob2\"),\n",
        "                 from_project_root(\"data/wl/wl.dev.iob2\"),\n",
        "                 from_project_root(\"data/wl/wl.test.iob2\")]\n",
        "```\n",
        "\n",
        "Add the pre-trained embedding path (line 289)\n",
        "```\n",
        "prepare_vocab(data_urls, from_project_root(\"data/embedding/cwlce.vec\"), update=True, min_count=1)\n",
        "```\n",
        "\n",
        "Change these three variables (lines 15-17):\n",
        "```\n",
        "LABEL_IDS = {\"neither\": 0, \"Disease\": 1, \"Finding\": 2, \"Medication\": 3, \"Procedure\": 4, \"Family_Member\": 5,\"Body_Part\": 6, \"Abbreviation\": 7}\n",
        "PRETRAINED_URL = from_project_root(\"data/embedding/cwlce.vec\")\n",
        "LABEL_LIST = {\"O\", \"Disease\", \"Finding\", \"Medication\", \"Procedure\", \"Family_Member\",\"Body_Part\", \"Abbreviation\"}\n",
        "```\n",
        "\n",
        "Finally, in \"model.py\", change the CharLSTM class, use the following:\n",
        "\n",
        "```\n",
        "class CharLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_chars, embedding_size, hidden_size, lstm_layers=1, bidirectional=True):\n",
        "        super().__init__()\n",
        "        self.n_chars = n_chars\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_hidden = hidden_size * (1 + bidirectional)\n",
        "\n",
        "        self.embedding = nn.Embedding(n_chars, embedding_size, padding_idx=0)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_size,\n",
        "            hidden_size=hidden_size,\n",
        "            bidirectional=bidirectional,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def sent_forward(self, words, lengths, indices):\n",
        "        sent_len = words.shape[0]\n",
        "        # words shape: (sent_len, max_word_len)\n",
        "\n",
        "        embedded = self.embedding(words)\n",
        "        # in_data shape: (sent_len, max_word_len, embedding_dim)\n",
        "\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu().numpy(), batch_first=True)\n",
        "        _, (hn, _) = self.lstm(packed)\n",
        "        # shape of hn:  (n_layers * n_directions, sent_len, hidden_size)\n",
        "\n",
        "        hn = hn.permute(1, 0, 2).contiguous().view(sent_len, -1)\n",
        "        hn2 = hn.clone()\n",
        "        # shape of hn:  (sent_len, n_layers * n_directions * hidden_size) = (sent_len, 2*hidden_size)\n",
        "\n",
        "        # shape of indices: (sent_len, max_word_len)\n",
        "        hn[indices] = hn2  # unsort hn\n",
        "        # unsorted = hn.new_empty(hn.size())\n",
        "        # unsorted.scatter_(dim=0, index=indices.unsqueeze(-1).expand_as(hn), src=hn)\n",
        "        return hn\n",
        "\n",
        "    def forward(self, sentence_words, sentence_word_lengths, sentence_word_indices):\n",
        "        # sentence_words [batch_size, *sent_len, max_word_len]\n",
        "        # sentence_word_lengths [batch_size, *sent_len]\n",
        "        # sentence_word_indices [batch_size, *sent_len, max_word_len]\n",
        "\n",
        "        batch_size = len(sentence_words)\n",
        "        batch_char_feat = torch.nn.utils.rnn.pad_sequence(\n",
        "            [self.sent_forward(sentence_words[i], sentence_word_lengths[i], sentence_word_indices[i])\n",
        "             for i in range(batch_size)], batch_first=True)\n",
        "\n",
        "        return batch_char_feat\n",
        "        # (batch_size, sent_len, 2 * hidden_size)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl6KX2rOyzOA"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gEl-SyIcxbf"
      },
      "source": [
        "Chilean Waiting List: Replace eval.py with the one we passed in the supplementary material (eval-2.py). This edited file writes the result of the predictions in a text file, which can be used to calculate task-specific metrics. The name of the file is boundary_result.txt. For the case of the waiting list dataset it is essential to do this since there are multilabel entities that the model cannot capture. \n",
        "\n",
        "Genia and Germeval: Simply take the functions from the metrics notebook and calculate them in eval.py, this is done in the eval-3.py file.\n",
        "\n",
        "\n",
        "In both cases, change the following lines, according to your dataset (just in testing step). In the \"model\" folder you will find the best model save.\n",
        "\n",
        "```\n",
        "model_url = from_project_root(\"data/model/end2end_model_epochnumber_score.pt\")\n",
        "test_url = from_project_root(\"data/wl/wl.test.iob2\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nFyH0vfQdYD"
      },
      "source": [
        "!python eval.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4NzINPiNc19"
      },
      "source": [
        "# ***Recursive-CRF model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2K9Ldn1WoFD"
      },
      "source": [
        "%cd mlc-paper-baselines/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zFIZr9yNHvj"
      },
      "source": [
        "# Clone the project from the unofficial repository, if you have already cloned it skip this step.\n",
        "!git clone https://github.com/yahshibu/nested-ner-tacl2020-flair.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLTpyE52XnOc"
      },
      "source": [
        "%cd nested-ner-tacl2020-flair/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyMf5YbDB83i"
      },
      "source": [
        "# First we create the folder where we are going to place the pre-trained word embeddings.\n",
        "!mkdir embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rKxC3UVDipZ"
      },
      "source": [
        "!pip install adabound\n",
        "!pip install flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRnYhffDOmEh"
      },
      "source": [
        "4. **Code fixes** \n",
        "\n",
        "- Add to the folder the files: gen_data_for_wl.py and gen_data_for_germ.py\n",
        "\n",
        "- Change the reader.py and embeddings.py files to the files given.\n",
        "\n",
        "- In crf.py change device problem if you are using a gpu, add the following line: indices_3 = indices_3.cuda() before line 300.\n",
        "\n",
        "All these changes were necessary, otherwise we could not run the code from the repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdkM5GjyGHYX"
      },
      "source": [
        "# We train, and then the file with the predictions will be saved in the \"dumps\" folder.\n",
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4Bqi0G5qsk-"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AfGljs0pt-f"
      },
      "source": [
        "!git clone https://github.com/yahshibu/nested-ner-tacl2020-flair.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_KSPIPhqugJ"
      },
      "source": [
        "%cd nested-ner-tacl2020-flair/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzpt40Ggqxrc"
      },
      "source": [
        "!python gen_data_for_genia.py\n",
        "!python gen_data_for_germ.py\n",
        "!python gen_data_for_wl.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRG4unlGsmKu"
      },
      "source": [
        "#%cd embeddings/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj21NJC2sfaE"
      },
      "source": [
        "# If you don't have the embeddings in binary format\n",
        "#from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "#model = KeyedVectors.load_word2vec_format('cwlce.vec', binary=False)\n",
        "#model.save_word2vec_format('cwlce.bin', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohpHw7Zcu35i"
      },
      "source": [
        "#%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWgF80Xtsx-Y"
      },
      "source": [
        "# We train and the predictions will be saved in the dumps folder.\n",
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}